{
  "bittensor_id": "distributed_training",
  "name": "DistributedTraining",
  "github": ["https://github.com/KMFODA/DistributedTraining"],
  "hw_requirements": "",
  "image_url": "",
  "description": "description.html",
  "bittensor_discord_id": "",
  "team": "KMFODA",
  "summary": "DistributedTraining is focused on training a single LLM in a distributed setting. Contributors are incentivized to provide compute, bandwidth, and low latency, using compute to train local models and bandwidth and latency to facilitate the averaging of model weights across participants.",
  "websites": [
    {
      "label": "website",
      "url": "https://distributed-training.notion.site/Decentralised-Distributed-Training-fd21bdfa72294dfeab8fb092770212b9"
    },
    {
      "label": "dashboard",
      "url": "https://wandb.ai/kmfoda/distributed_training"
    }
  ]
}
